# Importamos las librerías necesarias
from pyspark.sql import SparkSession, functions as F

# Inicializa la sesión de Spark
spark = SparkSession.builder.appName('Tarea3').getOrCreate()

# Define la ruta del archivo .csv en HDFS
file_path = 'hdfs://localhost:9000/Tarea3/mxk5-ce6w.csv'

# Lee el archivo .csv
df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(file_path)

# Imprime el esquema del DataFrame
df.printSchema()

# Muestra las primeras filas del DataFrame
df.show()

# Estadísticas básicas
df.summary().show()

# Filtra los registros con valor mayor a 5000 y selecciona columnas específicas
print("Registros con valor mayor a 5000\n")
dias = df.filter(F.col('valor') > 5000).select(
    'tipo_entidad', 'nombre_tipo_entidad', 'codigo_entidad', 
    'nombre_entidad', 'fecha_corte', 'cuenta', 
    'nombre_cuenta', 'moneda', 'nombre_moneda', 'signo_valor', 'valor'
)
dias.show()

# Ordena los registros por las columnas especificadas en el orden deseado
print("Valores ordenados\n")
sorted_df = df.orderBy(
    F.col("tipo_entidad").asc(),
    F.col("codigo_entidad").asc(),
    F.col("cuenta").asc(),
    F.col("moneda").asc(),
    F.col("fecha_corte").desc()
)
sorted_df.show()